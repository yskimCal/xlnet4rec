{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "XLNet4Rec_Beauty.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu5XLyDq3aHL"
      },
      "source": [
        "#XLNet4Rec - Amazon Beauty Ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyHqXGsR3VZy"
      },
      "source": [
        "#### References: \n",
        "<br> 1) Tokenization and embedding\n",
        "* https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313\n",
        "* https://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface/\n",
        "\n",
        "<br> 2) Training and Testing\n",
        "* https://mccormickml.com/2019/09/19/XLNet-fine-tuning/\n",
        "* https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912\n",
        "* https://huggingface.co/transformers/model_doc/xlnet.html\n",
        "\n",
        "<br> 3) Evaluation\n",
        "* https://github.com/FeiSun/BERT4Rec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn8F6r2LjJzq"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXGS7UZJdAbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51aea16-5fa7-4efc-b430-8b9b08edb9c7"
      },
      "source": [
        "# Install transformers\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 16.0MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 13.9MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 12.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 317kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 327kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 337kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 348kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 358kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 368kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 378kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 389kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 399kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 409kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 419kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 430kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 440kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 450kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 460kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 471kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 481kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 491kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 501kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 512kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 522kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 532kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 542kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 552kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 563kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 573kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 583kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 593kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 604kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 614kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 624kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 634kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 645kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 655kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 665kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 675kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 686kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 696kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 706kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 716kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 727kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 737kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 747kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 757kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 768kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 778kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 788kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 798kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 808kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 819kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 829kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 839kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 849kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 860kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 870kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 880kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 890kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 901kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 911kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 921kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 931kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 942kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 952kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 962kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 972kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 983kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 993kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=53c8f40e202c08f2583e714e29a3a4367813de87347db8c8dcd7ae55dd80befb\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AA5fyHyvWkq"
      },
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import io\n",
        "import pickle\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "import sklearn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import BertTokenizer, XLNetTokenizer, XLNetConfig, XLNetModel, XLNetLMHeadModel \n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfQJHgQqWlPs",
        "outputId": "c5165cb4-3d29-45e1-d82b-8acdbb253617"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5lKDtmdWsOB"
      },
      "source": [
        "excelfile = F\"/content/gdrive/My Drive/ratings_Beauty.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Cs12QdJYvsQd",
        "outputId": "58da3edc-6946-45c3-ded3-876050b2d3ec"
      },
      "source": [
        "# Visualize dataset\n",
        "amazon_ratings = pd.read_csv(excelfile)\n",
        "amazon_ratings = amazon_ratings.dropna()\n",
        "amazon_ratings.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A39HTATAQ9V7YF</td>\n",
              "      <td>0205616461</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1369699200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A3JM6GV9MNOF9X</td>\n",
              "      <td>0558925278</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1355443200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A1Z513UWSAAO0F</td>\n",
              "      <td>0558925278</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1404691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A1WMRR494NWEWV</td>\n",
              "      <td>0733001998</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1382572800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A3IAAVS479H7M7</td>\n",
              "      <td>0737104473</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1274227200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           UserId   ProductId  Rating   Timestamp\n",
              "0  A39HTATAQ9V7YF  0205616461     5.0  1369699200\n",
              "1  A3JM6GV9MNOF9X  0558925278     3.0  1355443200\n",
              "2  A1Z513UWSAAO0F  0558925278     5.0  1404691200\n",
              "3  A1WMRR494NWEWV  0733001998     4.0  1382572800\n",
              "4  A3IAAVS479H7M7  0737104473     1.0  1274227200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9MOEdOv-aE"
      },
      "source": [
        "# Find users with more than 5 ratings\n",
        "idlist = amazon_ratings['UserId'].value_counts()\n",
        "indexes = idlist[idlist >= 5].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSiKwsAZ9pUq"
      },
      "source": [
        "# Extract Popular items\n",
        "pdlist = amazon_ratings['ProductId'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcB7fXGrwDcX",
        "outputId": "a0340feb-6c88-48da-a58d-0884591470ef"
      },
      "source": [
        "# Combine users and ratings as a list\n",
        "ratings_subset = amazon_ratings[amazon_ratings['UserId'].isin(indexes)]\n",
        "ratings_subset.sort_values(by=['UserId', 'Timestamp'], inplace = True)\n",
        "ratings_final = ratings_subset.groupby('UserId')['ProductId'].apply(list).reset_index(name='items')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "FX50bwVuwKRz",
        "outputId": "dda698f6-d759-49bd-b3ba-4cbce7fa6a3e"
      },
      "source": [
        "# Check length of items per user\n",
        "import seaborn as sns\n",
        "\n",
        "item_lengths = []\n",
        "\n",
        "for r in range(0,len(ratings_final['items'])):\n",
        "\n",
        "    # get rough token count distribution\n",
        "    item_lengths.append(len(ratings_final['items'][r]))\n",
        "\n",
        "doc_lengths = np.array(item_lengths)\n",
        "\n",
        "sns.distplot(item_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0634e2aa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mILIc4GJwN0y",
        "outputId": "c2621d17-762f-41a2-e79e-fb1f8c2849f4"
      },
      "source": [
        "# Maximum length of items\n",
        "np.max(item_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "389"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQuo5qQawPsn",
        "outputId": "e11beaad-7faf-4dc4-c0fa-8acf70a84914"
      },
      "source": [
        "# Median length of items\n",
        "np.mean(item_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.96954595791805"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RMMerSjpwQY4",
        "outputId": "fcb4d261-3aa1-4ae2-dfad-e7fde0e3fa51"
      },
      "source": [
        "# Viusalize final dataset\n",
        "ratings_final.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>items</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00414041RD0BXM6WK0GX</td>\n",
              "      <td>[B007IY97U0, B00870XLDS, B008MIRO88, B00BQYYMN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00473363TJ8YSZ3YAGG9</td>\n",
              "      <td>[B000FABN7E, B0019LVFI0, B00AE07JIM, B0020HEBX...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00700212KB3K0MVESPIY</td>\n",
              "      <td>[B001MP0T2Q, B001RMP7M6, B003TMO3EU, B00028M3N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00728291XHL0K3ETGQOG</td>\n",
              "      <td>[B007Y7WL4A, B008FX4LO0, B009Q1YPVY, B003JM0YP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A0078719IR14X3NNUG0F</td>\n",
              "      <td>[B006L1DNWY, B008RRX9CE, B0069FDR96, B00BNB38A...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  UserId                                              items\n",
              "0  A00414041RD0BXM6WK0GX  [B007IY97U0, B00870XLDS, B008MIRO88, B00BQYYMN...\n",
              "1  A00473363TJ8YSZ3YAGG9  [B000FABN7E, B0019LVFI0, B00AE07JIM, B0020HEBX...\n",
              "2  A00700212KB3K0MVESPIY  [B001MP0T2Q, B001RMP7M6, B003TMO3EU, B00028M3N...\n",
              "3  A00728291XHL0K3ETGQOG  [B007Y7WL4A, B008FX4LO0, B009Q1YPVY, B003JM0YP...\n",
              "4   A0078719IR14X3NNUG0F  [B006L1DNWY, B008RRX9CE, B0069FDR96, B00BNB38A..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8n8aXffsQ8J",
        "outputId": "160655f4-31bf-4c50-d474-bd0073cc70e2"
      },
      "source": [
        "# Length of final dataset\n",
        "len(ratings_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52374"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyVdCnuCxgyf"
      },
      "source": [
        "# Create new tokens\n",
        "from itertools import chain\n",
        "#new_tokens = list(set(list(chain(*ratings_final['items']))))\n",
        "new_tokens = list(pdlist.index)[0:54541]\n",
        "#new_tokens = ['<unk>','<s>','</s>','<cls>','<sep>','<pad>','<mask>','<eod>','<eop>'] + new_tokens\n",
        "new_tokens = ['<unk>','<mask>','<pad>','<cls>','<sep>'] + new_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8fbbYL2x2kP",
        "outputId": "408b23c5-2eb4-4d02-976a-ea369192ce75"
      },
      "source": [
        "# Length of new tokens\n",
        "len(new_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54546"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAxh3aPgywQG"
      },
      "source": [
        "# Create vocab list for tokenizer\n",
        "with open('vocab.txt', 'w') as f:\n",
        "    for item in new_tokens:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sYDHwKawid-"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv1YJnfCfTLm"
      },
      "source": [
        "# Tokenizer\n",
        "tokenizer = BertTokenizer(vocab_file='vocab.txt', unk_token='<unk>', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL6oV44Xhsq3"
      },
      "source": [
        "#tokenizer2 = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LaNZVa1hyga"
      },
      "source": [
        "#tokenizer2.convert_tokens_to_ids('<pad>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB-v4XN8q1PF"
      },
      "source": [
        "# Padding function\n",
        "max_length=50\n",
        "# Now we can pad reply and distractor inputs and targets to the same length\n",
        "def pad(x, padding):\n",
        "    return x + [padding] * (max_length - len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gah1O_uTot4j"
      },
      "source": [
        "# Prepare item seq and padding function\n",
        "item_seq = ratings_final['items']\n",
        "train = []\n",
        "val = []\n",
        "test = []\n",
        "\n",
        "for i in range(0,len(item_seq)):\n",
        "  if len(item_seq[i]) > max_length:\n",
        "    seq = item_seq[i][0:max_length]\n",
        "  else:\n",
        "    seq = item_seq[i]\n",
        "  train.append(seq[0:-2])\n",
        "  val.append(seq[0:-1])\n",
        "  test.append(seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "8dJwbhKBqYxs",
        "outputId": "5c745cf8-b575-4788-e020-fa74afd43fef"
      },
      "source": [
        "'''\n",
        "# Testing\n",
        "seq = item_seq[0]\n",
        "tokens = seq + ['<sep>'] + ['<cls>'] \n",
        "\n",
        "# words token\n",
        "input_words = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_words = pad(input_words, 5)\n",
        "words = torch.tensor(input_words, dtype=torch.long)\n",
        "assert len(words) == max_length\n",
        "print(words)\n",
        "\n",
        "# lables\n",
        "lables = words[torch.where(words == 4)[0]-1]\n",
        "#self.labels.append(labels)\n",
        "print(lables)\n",
        "    \n",
        "# perm_mask\n",
        "perm_mask = torch.zeros((1, words.shape[0], words.shape[0]), dtype=torch.long)\n",
        "perm_mask[:, :, torch.where(words == 4)[0]-1] = 1\n",
        "#self.perm_mask.append(perm_mask)\n",
        "print(perm_mask[0])\n",
        "\n",
        "# target mapping\n",
        "target_mapping = torch.zeros((1, 1, words.shape[0]), dtype=torch.long)\n",
        "target_mapping[0, 0, torch.where(words == 4)[0]-1] = 1\n",
        "#self.perm_mask.append(target_mapping)\n",
        "print(target_mapping)\n",
        "\n",
        "# masking\n",
        "words[torch.where(words == 4)[0]-1] = 1\n",
        "#self.input_ids.append(words)\n",
        "print(words)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Testing\\nseq = item_seq[0]\\ntokens = seq + ['<sep>'] + ['<cls>'] \\n\\n# words token\\ninput_words = tokenizer.convert_tokens_to_ids(tokens)\\ninput_words = pad(input_words, 5)\\nwords = torch.tensor(input_words, dtype=torch.long)\\nassert len(words) == max_length\\nprint(words)\\n\\n# lables\\nlables = words[torch.where(words == 4)[0]-1]\\n#self.labels.append(labels)\\nprint(lables)\\n    \\n# perm_mask\\nperm_mask = torch.zeros((1, words.shape[0], words.shape[0]), dtype=torch.long)\\nperm_mask[:, :, torch.where(words == 4)[0]-1] = 1\\n#self.perm_mask.append(perm_mask)\\nprint(perm_mask[0])\\n\\n# target mapping\\ntarget_mapping = torch.zeros((1, 1, words.shape[0]), dtype=torch.long)\\ntarget_mapping[0, 0, torch.where(words == 4)[0]-1] = 1\\n#self.perm_mask.append(target_mapping)\\nprint(target_mapping)\\n\\n# masking\\nwords[torch.where(words == 4)[0]-1] = 1\\n#self.input_ids.append(words)\\nprint(words)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHyZXjE6q6z-"
      },
      "source": [
        "# Class for creating dataset\n",
        "class XLNetDataset(Dataset):\n",
        "\n",
        "  def __init__(self, item_seq, tokenizer, max_length=50):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.lables = []\n",
        "    #self.perm_mask = []\n",
        "    self.target_mapping = []\n",
        "    \n",
        "    for seq in item_seq: \n",
        "      if len(seq) == max_length:\n",
        "        seq = seq[0:-2]\n",
        "      elif len(seq) == (max_length-1):\n",
        "        seq = seq[0:-1]\n",
        "      else:\n",
        "        seq = seq\n",
        "      tokens = seq + ['<sep>'] + ['<cls>'] \n",
        "\n",
        "      # words token\n",
        "      input_words = tokenizer.convert_tokens_to_ids(tokens)\n",
        "      input_words = pad(input_words, 2)\n",
        "      words = torch.tensor(input_words, dtype=torch.long)\n",
        "      assert len(words) == max_length\n",
        "\n",
        "      # lables\n",
        "      lables = words[torch.where(words == 4)[0]-1]\n",
        "      self.lables.append(lables)\n",
        "\n",
        "      '''\n",
        "      # perm_mask\n",
        "      perm_mask = torch.zeros((1, words.shape[0], words.shape[0]), dtype=torch.long)\n",
        "      perm_mask[:, :, torch.where(words == 4)[0]-1] = 1\n",
        "      self.perm_mask.append(perm_mask[0])\n",
        "      '''\n",
        "      # target mapping\n",
        "      target_mapping = torch.zeros((1, 1, words.shape[0]), dtype=torch.float)\n",
        "      target_mapping[0, 0, torch.where(words == 4)[0]-1] = 1\n",
        "      self.target_mapping.append(target_mapping[0])\n",
        "\n",
        "      # masking\n",
        "      words[torch.where(words == 4)[0]-1] = 1\n",
        "      self.input_ids.append(words)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.lables[idx], self.target_mapping[idx], #self.perm_mask[idx], "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAmXd7x_rJkX"
      },
      "source": [
        "# Create Dataset\n",
        "#dataset = XLNetDataset(item_seq, tokenizer, max_length=50)\n",
        "train_dataset = XLNetDataset(train, tokenizer, max_length=50)\n",
        "val_dataset = XLNetDataset(val, tokenizer, max_length=50)\n",
        "test_dataset = XLNetDataset(test, tokenizer, max_length=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr4sUNlYn02i"
      },
      "source": [
        "# Reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJqqX1Mn-2dB"
      },
      "source": [
        "'''\n",
        "# Sample\n",
        "tokenizer2 = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "model2 = XLNetLMHeadModel.from_pretrained('xlnet-large-cased', return_dict=True)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4KukEtF_ARU",
        "outputId": "602bac6f-a5d6-4e47-f8dd-35102642f5ea"
      },
      "source": [
        "'''\n",
        "input_ids = torch.tensor(tokenizer2.encode(\"Hello, my dog is very <mask>\", add_special_tokens=False)).unsqueeze(0)  # We will predict the masked token\n",
        "input_ids\n",
        "#outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n",
        "#next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   17, 11368,    19,    94,  2288,    27,   172,     6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y3T5Ba00h4d",
        "outputId": "8a92acab-6dd7-4bb1-f557-fa358c5f3e63"
      },
      "source": [
        "#torch.tensor(tokenizer2.encode(\"cute\", add_special_tokens=False)).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10920]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv33ocls0Wh5",
        "outputId": "b42d9473-3d1b-4f81-a9b8-d24e7f8e1f54"
      },
      "source": [
        "#labels = torch.tensor(tokenizer2.encode(\"cute\", add_special_tokens=False)).unsqueeze(0)\n",
        "#labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10920]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWHXkWxelZQv",
        "outputId": "6cc4ab70-db5e-4d62-f8fb-509bf4b50f77"
      },
      "source": [
        "'''\n",
        "perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
        "perm_mask\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv_aPCSqnycD",
        "outputId": "5ee5a71b-0f8e-4a88-dd9c-71763027ac2e"
      },
      "source": [
        "'''\n",
        "target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] => let's predict one token\n",
        "target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n",
        "target_mapping\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0., 0., 0., 0., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnykLBWXHB27"
      },
      "source": [
        "'''\n",
        "outputs = model2(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n",
        "print(outputs)  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZMhyKucCHPa"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyDgGmc-CW_-"
      },
      "source": [
        "batch_size = 2\n",
        "\n",
        "# Create the DataLoaders for our training and validation datasets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = SequentialSampler(train_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = 1 # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "# Test\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, # The test samples.\n",
        "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlDSqSUaTz0a"
      },
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oNMWcNFXreO"
      },
      "source": [
        "config = XLNetConfig(\n",
        "    vocab_size=54546\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l8fK3F5XLhV"
      },
      "source": [
        "model = XLNetLMHeadModel(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pauqIJX1Wr_Z"
      },
      "source": [
        "'''\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vri-UoSOWtMz"
      },
      "source": [
        "'''\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                     lr=2e-5)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n_Qiy-f6mrp"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "WARMUP_STEPS = 100\n",
        "MAX_SEQ_LEN = 50\n",
        "total_steps = len(train_dataloader) * EPOCHS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMjzWAvU3ocC"
      },
      "source": [
        "'''\n",
        "# Testing\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps ,last_epoch = -1)\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  b_input_ids = batch[0].to(device)\n",
        "  b_labels = batch[1].to(device)\n",
        "  b_target_mapping = batch[2].to(device)\n",
        "  # Clear out the gradients (by default they accumulate)\n",
        "  optimizer.zero_grad()\n",
        "  # Forward pass\n",
        "  outputs = model(input_ids=b_input_ids, labels=b_labels, target_mapping=b_target_mapping)\n",
        "  loss = outputs[0]\n",
        "  logits = outputs[1]\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(loss)\n",
        "  print(logits)\n",
        "  #model.zero_grad()\n",
        "  if step == 10:\n",
        "    break\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF94-etu6dhS",
        "outputId": "49f80d4a-5bea-485b-81f3-467c86d7d10d"
      },
      "source": [
        "%%time\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps ,last_epoch = -1)\n",
        "proc_seq_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "models_folder = \"trained_models\"\n",
        "if not os.path.exists(models_folder):\n",
        "    os.mkdir(models_folder)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "    \n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[1].to(device)\n",
        "        b_target_mapping = batch[2].to(device)\n",
        "        \n",
        "        outputs = model(input_ids=b_input_ids, labels=b_labels, target_mapping=b_target_mapping)\n",
        "\n",
        "        loss, logits = outputs[:2]                        \n",
        "        loss.backward()\n",
        "        sum_loss = sum_loss + loss.detach().data\n",
        "                       \n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0    \n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        if batch_count == 100:\n",
        "            print(f\"sum loss {sum_loss}\")\n",
        "            batch_count = 0\n",
        "            sum_loss = 0.0\n",
        "    \n",
        "    # Store the model after each epoch to compare the performance of them\n",
        "    torch.save(model.state_dict(), os.path.join(models_folder, f\"XLNet4Rec_{epoch}.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n",
            "sum loss 1922.1448974609375\n",
            "sum loss 1883.1368408203125\n",
            "sum loss 1839.5030517578125\n",
            "sum loss 1922.7640380859375\n",
            "sum loss 1940.249267578125\n",
            "sum loss 1871.23193359375\n",
            "sum loss 1960.0362548828125\n",
            "sum loss 1886.475830078125\n",
            "sum loss 1905.7127685546875\n",
            "sum loss 1898.388671875\n",
            "sum loss 1864.1259765625\n",
            "sum loss 1927.08642578125\n",
            "sum loss 1908.98388671875\n",
            "sum loss 1867.8594970703125\n",
            "sum loss 1920.5791015625\n",
            "sum loss 1962.9193115234375\n",
            "sum loss 1895.4705810546875\n",
            "sum loss 1905.1766357421875\n",
            "sum loss 1815.031982421875\n",
            "sum loss 1915.73779296875\n",
            "sum loss 1930.24267578125\n",
            "sum loss 1916.1710205078125\n",
            "sum loss 1859.7236328125\n",
            "sum loss 1822.3399658203125\n",
            "sum loss 1866.4078369140625\n",
            "sum loss 1933.278076171875\n",
            "sum loss 1892.3060302734375\n",
            "sum loss 1953.1534423828125\n",
            "sum loss 1935.8675537109375\n",
            "sum loss 1919.7156982421875\n",
            "sum loss 1951.60791015625\n",
            "sum loss 1964.863037109375\n",
            "sum loss 1919.988037109375\n",
            "sum loss 1978.4381103515625\n",
            "sum loss 2002.5152587890625\n",
            "sum loss 1935.6336669921875\n",
            "sum loss 1951.167724609375\n",
            "sum loss 1857.94189453125\n",
            "sum loss 1817.2001953125\n",
            "sum loss 1956.4534912109375\n",
            "sum loss 1922.8643798828125\n",
            "sum loss 2006.0296630859375\n",
            "sum loss 1943.7996826171875\n",
            "sum loss 1907.7274169921875\n",
            "sum loss 1958.186279296875\n",
            "sum loss 1878.047119140625\n",
            "sum loss 1867.019775390625\n",
            "sum loss 1952.5875244140625\n",
            "sum loss 1998.1986083984375\n",
            "sum loss 1892.87548828125\n",
            "sum loss 1971.2576904296875\n",
            "sum loss 1911.2025146484375\n",
            "sum loss 1884.18896484375\n",
            "sum loss 1942.228515625\n",
            "sum loss 1892.9019775390625\n",
            "sum loss 1950.3349609375\n",
            "sum loss 1906.6566162109375\n",
            "sum loss 1931.370849609375\n",
            "sum loss 1918.8050537109375\n",
            "sum loss 1930.4798583984375\n",
            "sum loss 1887.3841552734375\n",
            "sum loss 1861.3687744140625\n",
            "sum loss 1914.658935546875\n",
            "sum loss 1896.0731201171875\n",
            "sum loss 1983.3763427734375\n",
            "sum loss 1949.6640625\n",
            "sum loss 1873.47509765625\n",
            "sum loss 1843.334716796875\n",
            "sum loss 1788.0782470703125\n",
            "sum loss 1912.435546875\n",
            "sum loss 1871.3795166015625\n",
            "sum loss 2014.5313720703125\n",
            "sum loss 1901.6807861328125\n",
            "sum loss 1817.803466796875\n",
            "sum loss 1879.2562255859375\n",
            "sum loss 1935.298583984375\n",
            "sum loss 1868.878173828125\n",
            "sum loss 1842.311767578125\n",
            "sum loss 1873.4586181640625\n",
            "sum loss 1862.298095703125\n",
            "sum loss 1846.2298583984375\n",
            "sum loss 1915.267822265625\n",
            "sum loss 1919.55029296875\n",
            "sum loss 1945.6419677734375\n",
            "sum loss 1888.96240234375\n",
            "sum loss 1890.9571533203125\n",
            "sum loss 1902.5810546875\n",
            "sum loss 1937.09912109375\n",
            "sum loss 1876.790771484375\n",
            "sum loss 1950.6148681640625\n",
            "sum loss 1845.2568359375\n",
            "sum loss 1866.3526611328125\n",
            "sum loss 1923.12353515625\n",
            "sum loss 1885.559814453125\n",
            "sum loss 1859.3721923828125\n",
            "sum loss 1848.061279296875\n",
            "sum loss 1861.900146484375\n",
            "sum loss 1938.19775390625\n",
            "sum loss 1914.674072265625\n",
            "sum loss 1868.2918701171875\n",
            "sum loss 1982.9571533203125\n",
            "sum loss 1861.9166259765625\n",
            "sum loss 1881.132568359375\n",
            "sum loss 1852.7489013671875\n",
            "sum loss 1866.5443115234375\n",
            "sum loss 1905.990234375\n",
            "sum loss 1845.7117919921875\n",
            "sum loss 1864.0728759765625\n",
            "sum loss 1912.75048828125\n",
            "sum loss 1892.0771484375\n",
            "sum loss 1844.780029296875\n",
            "sum loss 1915.907470703125\n",
            "sum loss 1835.5592041015625\n",
            "sum loss 1836.7554931640625\n",
            "sum loss 1846.96240234375\n",
            "sum loss 1867.0203857421875\n",
            "sum loss 1819.0360107421875\n",
            "sum loss 1885.94091796875\n",
            "sum loss 1870.7208251953125\n",
            "sum loss 1864.7388916015625\n",
            "sum loss 1925.58984375\n",
            "sum loss 1829.92138671875\n",
            "sum loss 1827.602294921875\n",
            "sum loss 1846.8970947265625\n",
            "sum loss 1802.06396484375\n",
            "sum loss 1897.4603271484375\n",
            "sum loss 1891.440185546875\n",
            "sum loss 1890.219482421875\n",
            "sum loss 1802.7362060546875\n",
            "sum loss 1816.748291015625\n",
            "CPU times: user 1h 33min 57s, sys: 13min 4s, total: 1h 47min 2s\n",
            "Wall time: 1h 47min 42s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylfsAwxDs5G4"
      },
      "source": [
        "model_save_name = 'XLNet4Rec_Amazon_1.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUGZ_gQv7R4D"
      },
      "source": [
        "# Another Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVcYBseXs_v5",
        "outputId": "7cf892b2-7e40-49c8-9891-d1155b1416d9"
      },
      "source": [
        "model_save_name = 'XLNet4Rec_Amazon_4.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_5Ox_bG7fCP",
        "outputId": "4698476c-febe-42a9-8b62-3ca8f760f89b"
      },
      "source": [
        "%%time\n",
        "# Train one more epoch\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps ,last_epoch = -1)\n",
        "proc_seq_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "models_folder = \"trained_models\"\n",
        "if not os.path.exists(models_folder):\n",
        "    os.mkdir(models_folder)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "    \n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[1].to(device)\n",
        "        b_target_mapping = batch[2].to(device)\n",
        "        \n",
        "        outputs = model(input_ids=b_input_ids, labels=b_labels, target_mapping=b_target_mapping)\n",
        "\n",
        "        loss, logits = outputs[:2]                        \n",
        "        loss.backward()\n",
        "        sum_loss = sum_loss + loss.detach().data\n",
        "                       \n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0    \n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        if batch_count == 100:\n",
        "            print(f\"sum loss {sum_loss}\")\n",
        "            batch_count = 0\n",
        "            sum_loss = 0.0\n",
        "    \n",
        "    # Store the model after each epoch to compare the performance of them\n",
        "    torch.save(model.state_dict(), os.path.join(models_folder, f\"XLNet4Rec_{epoch}.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n",
            "sum loss 1523.27197265625\n",
            "sum loss 1454.9598388671875\n",
            "sum loss 1460.3375244140625\n",
            "sum loss 1492.487060546875\n",
            "sum loss 1580.1248779296875\n",
            "sum loss 1498.4080810546875\n",
            "sum loss 1591.8331298828125\n",
            "sum loss 1551.698974609375\n",
            "sum loss 1647.8756103515625\n",
            "sum loss 1574.5560302734375\n",
            "sum loss 1553.2171630859375\n",
            "sum loss 1667.1380615234375\n",
            "sum loss 1680.241943359375\n",
            "sum loss 1605.65283203125\n",
            "sum loss 1592.3609619140625\n",
            "sum loss 1717.9918212890625\n",
            "sum loss 1644.5399169921875\n",
            "sum loss 1651.5316162109375\n",
            "sum loss 1587.42919921875\n",
            "sum loss 1658.5223388671875\n",
            "sum loss 1641.979248046875\n",
            "sum loss 1634.0677490234375\n",
            "sum loss 1619.27880859375\n",
            "sum loss 1613.419189453125\n",
            "sum loss 1600.4117431640625\n",
            "sum loss 1707.8651123046875\n",
            "sum loss 1706.2078857421875\n",
            "sum loss 1673.057861328125\n",
            "sum loss 1653.172607421875\n",
            "sum loss 1707.023681640625\n",
            "sum loss 1679.5775146484375\n",
            "sum loss 1697.626220703125\n",
            "sum loss 1702.654052734375\n",
            "sum loss 1738.0567626953125\n",
            "sum loss 1734.018798828125\n",
            "sum loss 1686.334228515625\n",
            "sum loss 1704.9346923828125\n",
            "sum loss 1609.7724609375\n",
            "sum loss 1645.105712890625\n",
            "sum loss 1745.2347412109375\n",
            "sum loss 1719.5511474609375\n",
            "sum loss 1767.163818359375\n",
            "sum loss 1742.0325927734375\n",
            "sum loss 1723.054443359375\n",
            "sum loss 1767.5321044921875\n",
            "sum loss 1696.3858642578125\n",
            "sum loss 1666.96484375\n",
            "sum loss 1773.1396484375\n",
            "sum loss 1794.17529296875\n",
            "sum loss 1695.5784912109375\n",
            "sum loss 1805.7266845703125\n",
            "sum loss 1693.625244140625\n",
            "sum loss 1701.0970458984375\n",
            "sum loss 1800.1275634765625\n",
            "sum loss 1741.447509765625\n",
            "sum loss 1799.1090087890625\n",
            "sum loss 1686.1905517578125\n",
            "sum loss 1695.80908203125\n",
            "sum loss 1720.8009033203125\n",
            "sum loss 1737.3544921875\n",
            "sum loss 1681.742919921875\n",
            "sum loss 1679.618896484375\n",
            "sum loss 1674.549072265625\n",
            "sum loss 1716.8109130859375\n",
            "sum loss 1781.233154296875\n",
            "sum loss 1777.600341796875\n",
            "sum loss 1707.0291748046875\n",
            "sum loss 1665.17919921875\n",
            "sum loss 1662.267822265625\n",
            "sum loss 1771.6868896484375\n",
            "sum loss 1737.3231201171875\n",
            "sum loss 1825.471435546875\n",
            "sum loss 1725.4942626953125\n",
            "sum loss 1668.21435546875\n",
            "sum loss 1726.040771484375\n",
            "sum loss 1758.512939453125\n",
            "sum loss 1693.7921142578125\n",
            "sum loss 1696.4620361328125\n",
            "sum loss 1702.8179931640625\n",
            "sum loss 1730.6793212890625\n",
            "sum loss 1704.5213623046875\n",
            "sum loss 1771.9427490234375\n",
            "sum loss 1733.7752685546875\n",
            "sum loss 1773.741455078125\n",
            "sum loss 1771.6854248046875\n",
            "sum loss 1770.8792724609375\n",
            "sum loss 1767.2177734375\n",
            "sum loss 1792.8575439453125\n",
            "sum loss 1752.1346435546875\n",
            "sum loss 1778.546630859375\n",
            "sum loss 1770.3521728515625\n",
            "sum loss 1745.7930908203125\n",
            "sum loss 1814.84326171875\n",
            "sum loss 1754.0390625\n",
            "sum loss 1766.7474365234375\n",
            "sum loss 1758.3525390625\n",
            "sum loss 1761.1231689453125\n",
            "sum loss 1878.14697265625\n",
            "sum loss 1857.9537353515625\n",
            "sum loss 1768.0382080078125\n",
            "sum loss 1863.4705810546875\n",
            "sum loss 1772.0277099609375\n",
            "sum loss 1798.775146484375\n",
            "sum loss 1766.7301025390625\n",
            "sum loss 1799.138427734375\n",
            "sum loss 1835.5758056640625\n",
            "sum loss 1828.3753662109375\n",
            "sum loss 1804.7957763671875\n",
            "sum loss 1828.9371337890625\n",
            "sum loss 1741.8050537109375\n",
            "sum loss 1741.7655029296875\n",
            "sum loss 1820.013427734375\n",
            "sum loss 1735.9375\n",
            "sum loss 1741.9813232421875\n",
            "sum loss 1760.936767578125\n",
            "sum loss 1808.323486328125\n",
            "sum loss 1757.59423828125\n",
            "sum loss 1806.625244140625\n",
            "sum loss 1804.419189453125\n",
            "sum loss 1783.617919921875\n",
            "sum loss 1858.0823974609375\n",
            "sum loss 1736.0836181640625\n",
            "sum loss 1766.4248046875\n",
            "sum loss 1766.107177734375\n",
            "sum loss 1765.663330078125\n",
            "sum loss 1843.9520263671875\n",
            "sum loss 1863.5709228515625\n",
            "sum loss 1842.444580078125\n",
            "sum loss 1760.843505859375\n",
            "sum loss 1785.38525390625\n",
            "CPU times: user 1h 32min 48s, sys: 12min 50s, total: 1h 45min 38s\n",
            "Wall time: 1h 46min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoYx34lDbYAz"
      },
      "source": [
        "model_save_name = 'XLNet4Rec_Amazon_4.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GorJsuTr9AQo"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j_2R63rU38-"
      },
      "source": [
        "model_save_name = 'XLNet4Rec_Amazon_4.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ZtSmGoqAX8"
      },
      "source": [
        "# Top products\n",
        "vocab_size = 54541\n",
        "topProducts = [0,1,2,3,4] + tokenizer.convert_tokens_to_ids(list(pdlist.index)[0:vocab_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXU_bnHKqC68"
      },
      "source": [
        "# Probability of each items\n",
        "sum_value = np.sum([x for x in pdlist.values[0:vocab_size]])\n",
        "probs = [0,0,0,0,0,] + [value / sum_value for value in pdlist.values[0:vocab_size]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znUcVolDqDvo",
        "outputId": "622be7fd-b47e-478b-fdec-e30f0d2c034d"
      },
      "source": [
        "# Negative samples\n",
        "%%time\n",
        "np.random.seed(12345)\n",
        "neg_samples = []\n",
        "for j in range(0,len(test_dataset)):\n",
        "  samples = np.random.choice(topProducts, 100, replace=False, p=probs)\n",
        "  notinTest = [i for i in samples if i not in test_dataset[j][0][0:torch.where(test_dataset[j][0] == 1)[0]].tolist()]\n",
        "  notinTest = notinTest + test_dataset[j][1].tolist()\n",
        "  neg_samples.append(notinTest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9min 2s, sys: 23.6 s, total: 9min 25s\n",
            "Wall time: 9min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W114bmuqEgy",
        "outputId": "80df5f1d-ab77-4b25-b573-411d8c35f9e0"
      },
      "source": [
        "# Compute Lengths\n",
        "neg_samples_length = []\n",
        "for i in range(0,len(neg_samples)):\n",
        "  neg_samples_length.append(len(neg_samples[i]))\n",
        "\n",
        "print(np.min(neg_samples_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV2v6P5IqPjh",
        "outputId": "7a2d6aba-e2ee-4b53-a9d2-2c80cd30d6aa"
      },
      "source": [
        "%%time\n",
        "# Testing\n",
        "model = model.to(device)\n",
        "labels = []\n",
        "results = []\n",
        "\n",
        "for idx, batch in enumerate(test_dataloader):\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_labels = batch[1].to(device)\n",
        "    b_target_mapping = batch[2].to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids=b_input_ids, labels=b_labels, target_mapping=b_target_mapping)\n",
        "      logits = outputs[1]\n",
        "    \n",
        "    softmax_logits = torch.softmax(logits[0],dim=1)\n",
        "    softmax_logits = softmax_logits.detach().cpu().numpy()\n",
        "\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    labels.append(label_ids[0])\n",
        "\n",
        "    results.append(softmax_logits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 24min 7s, sys: 28.1 s, total: 24min 35s\n",
            "Wall time: 24min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BiDO4p0qNhl"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6554tbAHqQwo",
        "outputId": "5fe98513-646d-426c-acf5-204ca011c045"
      },
      "source": [
        "# Ranking\n",
        "%%time\n",
        "tens = []\n",
        "fives = []\n",
        "ones = []\n",
        "\n",
        "for i in range(0,len(results)):\n",
        "  top_idx = np.argsort(results[i][0][neg_samples[i]])[-10:]\n",
        "  top_values = [neg_samples[i][e] for e in top_idx]\n",
        "  top_values.reverse()\n",
        "  tens.append(top_values)\n",
        "  fives.append(top_values[0:5])\n",
        "  ones.append([top_values[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 622 ms, sys: 16 ms, total: 638 ms\n",
            "Wall time: 639 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWlTZX2OqT0-"
      },
      "source": [
        "# For HitRate\n",
        "matched_tens = []\n",
        "matched_fives = []\n",
        "matched_ones = []\n",
        "\n",
        "for i in range(0,len(labels)):\n",
        "  matched_tens.append(labels[i][0] in tens[i])\n",
        "  matched_fives.append(labels[i][0] in fives[i])\n",
        "  matched_ones.append(labels[i][0] in ones[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFtRqXxwqUlT"
      },
      "source": [
        "# For HitRate\n",
        "matched_tens = []\n",
        "matched_fives = []\n",
        "matched_ones = []\n",
        "\n",
        "for i in range(0,len(labels)):\n",
        "  matched_tens.append(labels[i][0] in tens[i])\n",
        "  matched_fives.append(labels[i][0] in fives[i])\n",
        "  matched_ones.append(labels[i][0] in ones[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTrPhYCRqVe5",
        "outputId": "57a64e27-2ea4-46f1-9974-f1b7aea61ad1"
      },
      "source": [
        "# HitRate\n",
        "print('HR@10:', sum(matched_tens)/len(labels))\n",
        "print('HR@5:', sum(matched_fives)/len(labels))\n",
        "print('HR@1:', sum(matched_ones)/len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HR@10: 0.06201550387596899\n",
            "HR@5: 0.061175392370260054\n",
            "HR@1: 0.059647916905334705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjYTRTNTqWLF"
      },
      "source": [
        "# For NDCG\n",
        "ndcg_tens = []\n",
        "ndcg_fives = []\n",
        "ndcg_ones = []\n",
        "\n",
        "for i in range(0,len(labels)):\n",
        "  for j in range(0,len(tens[i])):\n",
        "    if labels[i] == tens[i][j]:\n",
        "      ndcg_tens.append(1/np.log2(j+2))\n",
        "    else:\n",
        "      ndcg_tens.append(0)\n",
        "  for j in range(0,len(fives[i])):\n",
        "    if labels[i] == fives[i][j]:\n",
        "      ndcg_fives.append(1/np.log2(j+2))\n",
        "    else:\n",
        "      ndcg_fives.append(0)\n",
        "  for j in range(0,len(ones[i])):\n",
        "    if labels[i] == ones[i][j]:\n",
        "      ndcg_ones.append(1/np.log2(j+2))\n",
        "    else:\n",
        "      ndcg_ones.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct1NzbuOqXYN",
        "outputId": "2622e4b8-7868-481c-f8c2-34fc0eeade3d"
      },
      "source": [
        "# NDCG\n",
        "print('NDCG@10:', np.mean(ndcg_tens))\n",
        "print('NDCG@5:', np.mean(ndcg_fives))\n",
        "print('NDCG@1:', np.mean(ndcg_ones))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NDCG@10: 0.006070942010597363\n",
            "NDCG@5: 0.012087769897673009\n",
            "NDCG@1: 0.059647916905334705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4sXW8dmqYMK"
      },
      "source": [
        "# For MRR\n",
        "rr = []\n",
        "\n",
        "for i in range(0,len(labels)):\n",
        "  for j in range(0,len(tens[i])):\n",
        "    if labels[i] == tens[i][j]:\n",
        "      rr.append(1/(j+1))\n",
        "    else:\n",
        "      rr.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNztKQC4qY3T",
        "outputId": "43922d95-a093-4e95-ccf5-9b217ddd59b3"
      },
      "source": [
        "# MRR\n",
        "print('MRR', np.mean(rr)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MRR 0.006030680738837457\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}